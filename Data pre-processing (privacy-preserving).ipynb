{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing your data to preserve the privacy ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In privacy-preserving machine learning and data minging in a distributed data scenario, we are facing a very challenging problem that the existing solutions are not very feasible and scalable to apply in real-world datasets. \n",
    "The most popular method -secure multiparty computation- takes very high costs of time, communication, and computations. To solve this problem, I suggest we can classify privacy level of features before doing analysis might decrease the cost. Considering the privacy level, features are categoried to:\n",
    "1. Identifiable features\n",
    "2. (Quasi-)identifiable features (instances can be re-identified by combining with other features)\n",
    "3. Sensitive features (depends on domain. e.g. Health data)\n",
    "4. Others "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the same time, do a pre-selection on features can eliminate the risk of breaching personal privacy. For instance, removing outliers is one way to protect 'outstanding' instances in the dataset which can avoid instances being recognized. If the features are sensitive which needs anonymization, this code also provides K-means to do the generalization. Users are free to set how general the values they want. Additionally, this jupyter notebook automate the data pre-process procedure and applied ipywidget which supports a very user-friendly interface. Users can easily do the data pre-processing, detect and remove outliers, and anonymize the features. It it very easy to follow and use in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, following methods/steps are presented:\n",
    "* Step 0: Input your data. Missing values will be checked automatically. If missing values in the dataset are represented by certain characters (e.g. '?', 'missing','Nan',etc), you can indicate it when you upload the data. \n",
    "* Step 1: Deal with your missing values. You can choose to remove the column if it contains too many missing values. Or, fill the missing values with certain numbers, characters, etc. \n",
    "* Step 2: Detect outliers to avoid 'outstanding' instances being re-identified. Local Outlier Factor (LOF) method is used to detect outliears (you can choose to remove the outliers or not based on the detection results). You can customize he percentage of outliers and the number of neighbors according to your dataset. The advantage of LOF is that not only global outliers can be detected, but local outliers are also sensitive to LOF.\n",
    "* Step 3: Detect (Quasi-)identifiable features. This code provides two methods to detect (quasi-)identifiable features: 1) Check the diversity of the feature which means how many different values this feature contains; 2) Using decision tree or randon forest to detect which features can be used to easily identify people (based on its possibilities)\n",
    "* Step 4: Anonymize identifiable/sensitve features. Generalization as one method to anonymize values is implemented by using K-means. You can select which features you want to anonymize according to your data, domain, and questions you want to answer by the data. You can customize how 'general' the values will be for this sensitive feature. Additionally, before anonymization, you can choose to normalize your data by using standard, min-max, or robust scaler. In the end, your pre-processed data will be saved in your local computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn import linear_model\n",
    "from numpy.linalg import inv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from traitlets import traitlets\n",
    "from IPython.display import display\n",
    "from ipywidgets import Layout, Button, Box, interact, interactive, fixed, interact_manual, Checkbox, FloatSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adopt the ipywidget button function to get the outcome of previous button function\n",
    "class LoadedButton(widgets.Button):\n",
    "    \"\"\"A button that can holds a value as a attribute.\"\"\"\n",
    "\n",
    "    def __init__(self, value=None, *args, **kwargs):\n",
    "        super(LoadedButton, self).__init__(*args, **kwargs)\n",
    "        # Create the value attribute.\n",
    "        self.add_traits(value=traitlets.Any(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set ipywidget layout and style ### \n",
    "style = {'description_width': 'initial'}\n",
    "uniLayout = Layout(width='50%', height='30px')\n",
    "\n",
    "##### ##### #####\n",
    "init_file = 'diabetic_data.csv'\n",
    "pathiwg = widgets.Text(value=init_file, description='Data File Path: ', style=style,layout=uniLayout)\n",
    "\n",
    "##### ##### #####\n",
    "mis_charac = widgets.Text(value='?', description='Any special characters to present missing values: ', style=style,layout=uniLayout)\n",
    "\n",
    "##### ##### #####\n",
    "check_mis = widgets.Checkbox(value=False, description='Check the missing values', disabled=False,\\\n",
    "                             style=style, layout=uniLayout)\n",
    "\n",
    "##### ##### #####\n",
    "button1 = LoadedButton(description=\"Input data\", button_style='success', value=None)\n",
    "#widgets.Button(description=\"Input data\", button_style='success')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### check if there is any missing value in the dataset ###\n",
    "def check_missing(df, col):\n",
    "    missing  = 0\n",
    "    CheckNull = df.isnull().sum()\n",
    "    for var in range(0, len(CheckNull)):\n",
    "        if CheckNull[var] != 0:\n",
    "            print(col[var], '\\t', CheckNull[var])\n",
    "            missing = missing + 1\n",
    "\n",
    "    if missing == 0:\n",
    "        print('Dataset is complete with no blanks.')\n",
    "    else:\n",
    "        print('Totally, %d features have missing values (blanks).' %missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Input data function ###\n",
    "def input_data(event):\n",
    "    \n",
    "    file_path = pathiwg.value\n",
    "    df_ori =  pd.DataFrame.from_csv(file_path, index_col=None)\n",
    "    col = df_ori.columns\n",
    "    \n",
    "    if event != 'silent':\n",
    "        print('***** Step 0 *****')\n",
    "        print('Input data successfully!')\n",
    "\n",
    "        ##### Replace customized missing valve #####\n",
    "        mis_value_code = mis_charac.value\n",
    "        if len(mis_value_code) > 0 :\n",
    "            df_ori = df_ori.replace({mis_value_code : np.nan})\n",
    "\n",
    "        ##### Check missing values #####\n",
    "        if check_mis.value == True: \n",
    "            check_missing(df_ori, col)\n",
    "    print('****** Done ****** \\n')\n",
    "    event.value = df_ori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "411b812d3e864216bd3a5fa5a50ba50b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(Box(children=(Text(value='diabetic_data.csv', description='Data File Path: ', layout=Layou…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Step 0 *****\n",
      "Input data successfully!\n",
      "race \t 2273\n",
      "weight \t 98569\n",
      "payer_code \t 40256\n",
      "medical_specialty \t 49949\n",
      "diag_1 \t 21\n",
      "diag_2 \t 358\n",
      "diag_3 \t 1423\n",
      "Totally, 7 features have missing values (blanks).\n",
      "****** Done ****** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "group0 = [pathiwg, mis_charac, check_mis, button1]\n",
    "\n",
    "box_layout = Layout(display='center',\n",
    "                    flex_flow='column',\n",
    "                    align_items='stretch',\n",
    "                    align_content='flex-start',\n",
    "                    width='80%')\n",
    "box0 = widgets.Box(children=group0, layout=box_layout)\n",
    "accordion = widgets.Accordion(children=[box0], style=style)\n",
    "accordion.set_title(0, 'Step 0 Input data')\n",
    "button1.on_click(input_data)\n",
    "accordion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect local outlier which can be easily re-identify ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set ipywidget layout and style ### \n",
    "\n",
    "##### ##### #####\n",
    "df_ori = button1.value\n",
    "col = df_ori.columns\n",
    "col_forRemove = col.tolist()\n",
    "col_forRemove.insert(0, 'No columns to remove')\n",
    "remove_col = widgets.SelectMultiple(\n",
    "                                options=col_forRemove,\n",
    "                                value=[col_forRemove[0]],\n",
    "                                description='Remove certain columns: ',\n",
    "                                disabled=False,\n",
    "                                style=style,\n",
    "                                layout=Layout(width='50%', height='150px')\n",
    "                                )\n",
    "\n",
    "##### ##### ####\n",
    "fill_missing = widgets.Text(value='?', description='Fill missing value with certain character/num: ', \\\n",
    "                            style=style,layout=uniLayout)\n",
    "\n",
    "##### ##### #####\n",
    "button2 = LoadedButton(description=\"Process\", button_style='success', value = button1.value)\n",
    "\n",
    "##### ##### #####\n",
    "id_feature = widgets.SelectMultiple(\n",
    "                                options=col,\n",
    "                                value=[col[0]],\n",
    "                                description='Select ID/data index column: ',\n",
    "                                disabled=False,\n",
    "                                style=style,\n",
    "                                layout=Layout(width='50%', height='150px')\n",
    "                                )\n",
    "\n",
    "##### ##### #####\n",
    "ngb_wgt = widgets.BoundedIntText(value=50,min=1,step=1,description='Neighbors:',\\\n",
    "                                 disabled=False, style=style, layout=uniLayout)\n",
    "\n",
    "##### ##### #####\n",
    "perc_wgt = widgets.BoundedFloatText(value=0.05,min=0, max=1,step=1,description='Outliers percent:',\\\n",
    "                                    disabled=False, style=style, layout=uniLayout)\n",
    "\n",
    "##### ##### #####\n",
    "checkbox_remove = widgets.Checkbox(value=False, description='Remove detected outliers', disabled=False,\\\n",
    "                             style=style, layout=uniLayout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Replace string data #####\n",
    "def encode_string(df_ori):\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    obj = 0\n",
    "    colList = df_ori.columns\n",
    "    for i in range(0, len(colList)):\n",
    "        if df_ori.dtypes.values[i] == 'object':\n",
    "            obj = obj + 1\n",
    "            le.fit(df_ori[colList[i]].drop_duplicates()) \n",
    "            df_ori[colList[i]] = le.transform(df_ori[colList[i]])\n",
    "    return df_ori\n",
    "\n",
    "##### Outlier detection #####\n",
    "def detectOUT (df, IDs, neighbours, percent):\n",
    "    X = df.drop(IDs, axis=1)\n",
    "    clf = LocalOutlierFactor(n_neighbors=neighbours, contamination=percent, leaf_size=1)\n",
    "    y_pred = clf.fit_predict(X)\n",
    "    X_scores = clf.negative_outlier_factor_\n",
    "    return y_pred, X_scores\n",
    "\n",
    "##### Remove outliers #####\n",
    "def remove_outliers(df_ori, y_pred):\n",
    "    df_ori['outlier'] = pd.Series(y_pred)\n",
    "    df_inliers = df_ori[df_ori['outlier']==1]\n",
    "    df_inliers = df_inliers.drop('outlier', axis=1)\n",
    "\n",
    "    return df_inliers\n",
    "\n",
    "##### Remove empty or un-diverse features #####\n",
    "def removeCol(df, reCol):\n",
    "    new_df = df.drop(reCol, axis=1)\n",
    "    return new_df\n",
    "# Replace blank with new character\n",
    "def replaceBlank(df, new):\n",
    "    new_df = df.fillna(new)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(event2):\n",
    "    print(\"Step 2 Processing... \")\n",
    "    df_ori = event2.value\n",
    "    \n",
    "    ##### Remove certain columns\n",
    "    reCol = []\n",
    "    for i in remove_col.value:\n",
    "        if i != 'No columns to remove':\n",
    "            reCol.append(i)\n",
    "    if len(reCol) > 0:\n",
    "        df_ori = removeCol(df_ori, reCol)\n",
    "\n",
    "    ##### Fill missing values\n",
    "    if len(fill_missing.value) > 0:\n",
    "        df_ori = replaceBlank(df_ori, fill_missing.value)\n",
    "    \n",
    "    ##### String to numbers\n",
    "    df_ori = encode_string(df_ori)\n",
    "        \n",
    "    ##### Outlier detection #####\n",
    "    if perc_wgt.value > 0:\n",
    "        y_pred, X_scores = detectOUT (df_ori, list(id_feature.value), ngb_wgt.value, perc_wgt.value)\n",
    "        print('Number of outliers', Counter(y_pred).get(-1))\n",
    "        print('Number of inliers', Counter(y_pred).get(1))\n",
    "    \n",
    "    ##### Remove outliers #####\n",
    "    if checkbox_remove.value == True:\n",
    "        df_inliers = remove_outliers(df_ori, y_pred)\n",
    "        print('Outliers removed successfully!')\n",
    "        event2.value = df_inliers\n",
    "    else:\n",
    "        event2.value = df_ori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd669d8b5ab542488647be39bd2d563f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(Box(children=(Text(value='?', description='Fill missing value with certain character/num: …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "group1 = [fill_missing, remove_col]\n",
    "group2 = [id_feature, ngb_wgt, perc_wgt, checkbox_remove, button2]\n",
    "\n",
    "box1 = widgets.Box(children=group1, layout=box_layout)\n",
    "box2 = widgets.Box(children=group2, layout=box_layout)\n",
    "\n",
    "accordion1 = widgets.Accordion(children=[box1, box2], style=style)\n",
    "accordion1.set_title(0, 'Step 1 Pre-process')\n",
    "accordion1.set_title(1, 'Step 2 Detect potential outliers')\n",
    "\n",
    "button2.on_click(pre_process)\n",
    "accordion1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect (Potential) identifiable features ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Check diversity of features #####\n",
    "id_detect_wgt = widgets.RadioButtons(options=['Diversity Checking', \\\n",
    "                                               'Decision Tree', 'Random Forest'],\n",
    "                                        value='Diversity Checking',\n",
    "                                        description='Detect potential (quasi-)identifiable features:',\n",
    "                                        disabled=False,\n",
    "                                        style=style,\n",
    "                                        layout=Layout(width='80%', height='60px')\n",
    "                                        )\n",
    "\n",
    "##### Check diversity of features #####\n",
    "id_options = list(id_feature.value)\n",
    "id_col_wgt = widgets.RadioButtons(options=id_options,\n",
    "                                    value=id_options[0],\n",
    "                                    description='Select your ID column:',\n",
    "                                    disabled=False,\n",
    "                                    style=style,\n",
    "                                    layout=Layout(width='80%', height='60px')\n",
    "                                    )\n",
    "\n",
    "##### ##### #####\n",
    "rank_wgt = widgets.BoundedIntText(value=len(button2.value.columns),min=1,max=len(button2.value.columns),\\\n",
    "                                  step=1,description='Number of features to show:', disabled=False, style=style, layout=uniLayout)\n",
    "\n",
    "##### ##### #####\n",
    "button3 = LoadedButton(description=\"Process\", button_style='success', value = button2.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_feature_detect(event3):\n",
    "    print(\"Step 3 Processing... \")\n",
    "\n",
    "    df_inliers = event3.value\n",
    "    limitNum = rank_wgt.value\n",
    "    \n",
    "    if id_detect_wgt.index == 0:\n",
    "        ##### Check diversity of features #####\n",
    "        diver = []\n",
    "#         diver2 = []\n",
    "        for c in df_inliers.columns:\n",
    "            diver.append(len(Counter(df_inliers[c]).keys()))\n",
    "#             diver2.append(len(Counter(df_ori[c]).keys()))\n",
    "\n",
    "        diver_df = pd.DataFrame.from_records([df_inliers.columns, diver]).transpose()\n",
    "        diver_df.columns = ['Features', 'Inliers']\n",
    "        print(diver_df.sort_values(by=['Inliers'], ascending=False)[0:limitNum])\n",
    "    else:\n",
    "        tree_detection(id_detect_wgt.index, limitNum, df_inliers, id_col_wgt.value)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##### Feature Selection/ranking #####\n",
    "def tree_detection(index, limitNum, df_inliers, id_column):\n",
    "    # Build a forest and compute the feature importances\n",
    "    if index == 1:\n",
    "        regressor = DecisionTreeRegressor(min_impurity_decrease=0)\n",
    "    elif index == 2:\n",
    "        regressor = RandomForestRegressor(n_estimators=200,\n",
    "                                              random_state=0, \n",
    "                                            min_samples_split=2, \n",
    "                                            min_samples_leaf=1, \n",
    "                                            min_impurity_decrease=0)\n",
    "\n",
    "    y = df_inliers[id_column]\n",
    "    X = df_inliers.drop(id_column, axis=1)\n",
    "    X_col = X.columns\n",
    "    regressor.fit(X, y)\n",
    "    importances = regressor.feature_importances_\n",
    "\n",
    "    \n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    # Print the feature ranking\n",
    "    print(\"Feature ranking:\")\n",
    "\n",
    "    for f in range(0, limitNum): #X.shape[1]\n",
    "        print(\"%d. feature %s (%f)\" % (f + 1, X_col[indices[f]], importances[indices[f]]))\n",
    "        \n",
    "    if index == 2:\n",
    "        std = np.std([tree.feature_importances_ for tree in regressor.estimators_], axis=0)\n",
    "        # Plot the feature importances of the forest\n",
    "        plt.figure(figsize=(12,9))\n",
    "        plt.title(\"Feature importances\")\n",
    "        plt.bar(range(X.shape[1]), importances[indices],\n",
    "               color=\"r\", yerr=std[indices], align=\"center\")\n",
    "        plt.xticks(range(X.shape[1]), indices)\n",
    "        plt.xlim([-1, X.shape[1]])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quasi-identifiable features/attributes/variables ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Remove highly identifiable features #####\n",
    "df_ori = button2.value\n",
    "col = df_ori.columns\n",
    "col_forRemove = col.tolist()\n",
    "col_forRemove.insert(0, 'No columns to remove')\n",
    "remove_id_col = widgets.SelectMultiple(\n",
    "                                options=col_forRemove,\n",
    "                                value=[col_forRemove[0]],\n",
    "                                description='Exclude ID/categorical columns: ',\n",
    "                                disabled=False,\n",
    "                                style=style,\n",
    "                                layout=Layout(width='50%', height='150px')\n",
    "                                )\n",
    "\n",
    "##### ##### #####\n",
    "clu_method_wgt = widgets.RadioButtons(options=['None', 'K-means', 'K-means (extra distance column)'],\n",
    "                                    value='None',\n",
    "                                    description='Generalize features by using',\n",
    "                                    disabled=False,\n",
    "                                    style=style,\n",
    "                                    layout=Layout(width='80%', height='80px')\n",
    "                                    )\n",
    "\n",
    "\n",
    "##### ##### #####\n",
    "scaler_wgt = widgets.RadioButtons(options=['None', 'Standardization', 'Min-Max Scaler', 'Robust Scaler'],\n",
    "                                    value='None',\n",
    "                                    description='Normalize data by',\n",
    "                                    disabled=False,\n",
    "                                    style=style,\n",
    "                                    layout=Layout(width='80%', height='90px')\n",
    "                                    )\n",
    "\n",
    "\n",
    "##### Generalize (Quasi-) identifiable features #####\n",
    "clu_wgt = widgets.BoundedIntText(value=20,min=1,max=len(button2.value)/10,\\\n",
    "                                  step=1,description='Number of clusters:', disabled=False, style=style, layout=uniLayout)\n",
    "\n",
    "\n",
    "##### ##### #####\n",
    "saveiwg = widgets.Text(value='preprocessed_dataFile.csv', description='Save processed data: ', style=style,layout=uniLayout)\n",
    "\n",
    "##### ##### #####\n",
    "button4 = LoadedButton(description=\"Process\", button_style='success', value = button2.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### K-means clustering #####\n",
    "def kmeans_func (df_inliers_3, cluster_mode, scaler_mode, clusterNum):\n",
    "    \n",
    "    scaler = [None, StandardScaler(), MinMaxScaler(), RobustScaler()]\n",
    "    \n",
    "    if scaler_mode == 0:\n",
    "        scaled_df_inliers_3 = df_inliers_3\n",
    "    else:\n",
    "        scaled_df_inliers_3 = scaler[scaler_mode].fit_transform(df_inliers_3)\n",
    "    if cluster_mode > 0:\n",
    "        kmeans = KMeans(n_clusters=clusterNum, random_state=0).fit(scaled_df_inliers_3)\n",
    "        data_clusterLabels = kmeans.labels_\n",
    "        data_clusterCenters = kmeans.cluster_centers_\n",
    "\n",
    "        anonymized = []\n",
    "        dist_center = []\n",
    "        for i in range(0, len(data_clusterLabels)):\n",
    "            anonymized.append(data_clusterCenters[data_clusterLabels[i]])\n",
    "            if cluster_mode == 2:\n",
    "                dist_center.append(euclidean_distances([data_clusterCenters[data_clusterLabels[i]]], \\\n",
    "                                                  [scaled_df_inliers_3.iloc[i]])[0])\n",
    "        df_anonymized = pd.DataFrame.from_records(anonymized, columns=scaled_df_inliers_3.columns)\n",
    "\n",
    "        if cluster_mode == 2:\n",
    "            df_anonymized['dist_center'] = pd.Series(dist_center)\n",
    "\n",
    "        print(Counter(kmeans.labels_))\n",
    "    else:\n",
    "        df_anonymized = scaled_df_inliers_3\n",
    "        \n",
    "    print('Done!!')\n",
    "    return df_anonymized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anonymization(event4):\n",
    "    print('\\n\\n Step 4 Processing ...') \n",
    "    df_inliers_2 = event4.value\n",
    "\n",
    "    ##### Remove certain columns\n",
    "    reCol = []\n",
    "    for i in remove_id_col.value:\n",
    "        if i != 'No columns to remove':\n",
    "            reCol.append(i)\n",
    "            \n",
    "    if len(reCol) > 0:\n",
    "        df_inliers_3 = removeCol(df_inliers_2, reCol)\n",
    "    else:\n",
    "        df_inliers_3 = df_inliers_2\n",
    "        \n",
    "    ##### K-means clustering #####\n",
    "    df_anonymized = kmeans_func(df_inliers_3, clu_method_wgt.index, scaler_wgt.index, clu_wgt.value)\n",
    "    \n",
    "    replace_col = df_inliers_3.columns\n",
    "    for i in replace_col:\n",
    "        df_inliers_2[i] = df_anonymized[i]\n",
    "    \n",
    "    if clu_method_wgt.index == 0:\n",
    "        df_inliers_3.to_csv(saveiwg.value, sep=',', encoding='utf-8')\n",
    "        event4.value = df_inliers_3\n",
    "    else:\n",
    "        df_inliers_2.to_csv(saveiwg.value, sep=',', encoding='utf-8')\n",
    "        event4.value = df_inliers_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c04bb55794fc4d6b990c98cd7ebcb425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(Box(children=(RadioButtons(description='Detect potential (quasi-)identifiable features:', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "group3 = [id_detect_wgt, id_col_wgt, rank_wgt, button3]\n",
    "group4 = [remove_id_col, scaler_wgt, clu_method_wgt, clu_wgt, saveiwg, button4]\n",
    "\n",
    "box3 = widgets.Box(children=group3, layout=box_layout)\n",
    "box4 = widgets.Box(children=group4, layout=box_layout)\n",
    "\n",
    "accordion3 = widgets.Accordion(children=[box3, box4], style=style)\n",
    "accordion3.set_title(0, 'Step 3 Detect potential (quasi-)identifiable features')\n",
    "accordion3.set_title(1, 'Step 4 Anonymize (generalize) (quasi-)identifiable features')\n",
    "\n",
    "button3.on_click(id_feature_detect)\n",
    "button4.on_click(anonymization)\n",
    "accordion3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Clustering #####\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.cluster import MiniBatchKMeans\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# inpu = button2.value[['diag_2', 'diag_3']]\n",
    "\n",
    "# scalermm = MinMaxScaler(feature_range=(0,1))\n",
    "# scal_inpu = scalermm.fit_transform(inpu)\n",
    "\n",
    "# kmeans = KMeans(n_clusters=25, random_state=0).fit(scal_inpu)\n",
    "# # minikmeans = MiniBatchKMeans(n_clusters=20, random_state=0, batch_size=6).fit(scal_inpu)\n",
    "# print(Counter(kmeans.labels_))\n",
    "# print(kmeans.cluster_centers_)\n",
    "\n",
    "\n",
    "# plot the learned frontier, the points\n",
    "# xx, yy = np.meshgrid(np.linspace(0, 1, 100), np.linspace(0, 1, 100))\n",
    "\n",
    "# resMap = np.c_[xx.ravel(), yy.ravel()]\n",
    "# Z = kmeans.predict(resMap)\n",
    "\n",
    "# plt.figure(figsize=(8,8))\n",
    "# plt.scatter(resMap[:,0], resMap[:,1], c = Z, marker='o',  alpha=0.02)\n",
    "\n",
    "# plt.scatter(scal_inpu[:,0], scal_inpu[:,1], c = kmeans.labels_, marker='.',  alpha=0.5)\n",
    "# plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], marker='o')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitive features analysis SMC ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Level 1: Naive Bayes (all parties know target class) #####\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "scalermm = MinMaxScaler(feature_range=(1,2))\n",
    "inliers_sld = scalermm.fit_transform(df_inliers)\n",
    "df_inliers_sld = pd.DataFrame.from_records(inliers_sld)\n",
    "df_inliers_sld.columns = df_inliers.columns\n",
    "\n",
    "TargetClass = df_inliers['readmitted']\n",
    "df_pcd = df_inliers_sld.drop(['encounter_id', 'patient_nbr', 'readmitted'], axis=1)\n",
    "\n",
    "col_data = df_pcd.columns\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_pcd, TargetClass, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SA_train = X_train[col_data[0:20]]\n",
    "SA_test = X_test[col_data[0:20]]\n",
    "\n",
    "SB_train = X_train[col_data[20:40]]\n",
    "SB_test = X_test[col_data[20:40]]\n",
    "\n",
    "tot_train = pd.concat([SA_train, SB_train], axis=1, join='inner')#X_train[col_data[0:40]]\n",
    "tot_test = pd.concat([SA_test, SB_test], axis=1, join='inner')#X_test[col_data[0:40]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train three classifiers separately for A, B sites and entire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from scipy.special import comb, logsumexp\n",
    "\n",
    "gnb1 = GaussianNB()\n",
    "gnb1.fit(SA_train, y_train)\n",
    "\n",
    "gnb2 = GaussianNB()\n",
    "gnb2.fit(SB_train, y_train)\n",
    "\n",
    "gnb3 = GaussianNB()\n",
    "gnb3.fit(tot_train, y_train)\n",
    "# print(\"Number of mislabeled points out of a total %d points : %d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lh1 = gnb1._joint_log_likelihood(SA_train.iloc[0:2])\n",
    "lhl1 = logsumexp(lh1, axis=1)\n",
    "np.exp(lh1 - np.atleast_2d(lhl1).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lh2 = gnb2._joint_log_likelihood(SB_train.iloc[0:2])\n",
    "lhl2 = logsumexp(lh2, axis=1)\n",
    "np.exp(lh2 - np.atleast_2d(lhl2).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lh3 = gnb3._joint_log_likelihood(tot_train.iloc[0:2])\n",
    "lhl3 = logsumexp(lh3, axis=1)\n",
    "np.exp(lh3 - np.atleast_2d(lhl3).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lht = lh1 + lh2 - np.log(gnb1.class_prior_)\n",
    "\n",
    "lhlt = logsumexp(lht, axis=1)\n",
    "np.exp(lht - np.atleast_2d(lhlt).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joint_log_likelihood = []\n",
    "# for i in range(np.size(gnb1.classes_)):###\n",
    "#     jointi = np.log(gnb3.class_prior_[i])\n",
    "#     n_ij = - 0.5 * np.sum(np.log(2. * np.pi * np.append(gnb1.sigma_[i, :], gnb2.sigma_[i, :])))\n",
    "#     n_ij -= 0.5 * np.sum(((tot_train.iloc[0:2] - np.append(gnb1.theta_[i, :], gnb2.theta_[i, :]))**2)/np.append(gnb1.sigma_[i, :], gnb2.sigma_[i, :]), 1)\n",
    "#     joint_log_likelihood.append(jointi + n_ij)\n",
    "\n",
    "# joint_log_likelihood = np.array(joint_log_likelihood).T\n",
    "# joint_log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Level 2: Naive Bayes (partially have target class) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SA = df_inliers['age'].iloc[0:100]\n",
    "SB = df_inliers['readmitted'].iloc[0:100]\n",
    "SB_classes = list(Counter(SB).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "bi_SB = label_binarize(SB, classes=SB_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SA_mul = len(SA) * [1] # len(SA_train)\n",
    "SA_mul = np.multiply(SA_mul, SA)\n",
    "    \n",
    "SB_mul = len(bi_SB[:,0]) * [1] # len(SB_train)\n",
    "SB_mul = np.multiply(SB_mul, bi_SB[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SA_mul = len(df_inliers['age']) * [1] # len(SA_train)\n",
    "# for s in SA_train.columns:\n",
    "#     SA_mul = np.multiply(SA_mul, SA_train[s].values)\n",
    "    \n",
    "# SB_mul = len(df_inliers['diabetesMed']) * [1] # len(SB_train)\n",
    "# for s in SB_train.columns:\n",
    "#     SB_mul = np.multiply(SB_mul, SB_train[s].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaPro = np.dot(SA_mul, SB_mul)\n",
    "scaPro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "A_random = np.random.randint(0,5, len(SA_mul))\n",
    "np.random.seed(2)\n",
    "C_noise = np.random.randint(0,5, (len(SA_mul), len(SA_mul)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SA_noise = np.add(SA_mul, np.dot(C_noise, A_random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_noise = np.dot(SA_noise, SB_mul)  \n",
    "SB_noise = np.dot(C_noise.transpose(), SB_mul)\n",
    "SB_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "B_random_single = np.random.randint(0,5, int(len(SB_mul)/2)) #in this case r =5\n",
    "B_random = []\n",
    "for i in range(0, len(B_random_single)): \n",
    "    B_random.append(B_random_single[i])\n",
    "    B_random.append(B_random_single[i])\n",
    "B_noise_2 = SB_noise + B_random\n",
    "\n",
    "#B sends A: Snoise_1 and ynoise_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_sum = 0\n",
    "for i in range(0, len(B_random_single)):\n",
    "    rand_sum = rand_sum + (A_random[2*i] + A_random[2*i+1]) * B_random_single[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_noise - np.dot(A_random,B_noise_2) + rand_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Secure Sum ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.array([[0,0,0,0,0,0],[2,2,2,2,2,2],[3,3,3,3,3,3]])\n",
    "\n",
    "k = 2\n",
    "n = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkIt(seq, num):\n",
    "    avg = len(seq) / float(num)\n",
    "    out = []\n",
    "    last = 0.0\n",
    "\n",
    "    while last < len(seq):\n",
    "        out.append(seq[int(last):int(last + avg)])\n",
    "        last += avg\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB = []\n",
    "for i in range(0, len(D)):\n",
    "    DB.append(chunkIt(D[i], k))\n",
    "DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.zeros((3, 2, 3))\n",
    "\n",
    "for rc in range(k, 0, -1):\n",
    "    for j in range(0, k):\n",
    "        for i in range(0, n):\n",
    "            s[i][j] = DB[i][j] + s[i][j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Privacy Preserving Linear Regression ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from numpy.linalg import inv\n",
    "\n",
    "df_inliers = button2.value\n",
    "### Centralized data ###\n",
    "X = df_inliers[['age','insulin','time_in_hospital', 'diag_1', 'diag_2']].iloc[0:1000]\n",
    "b0_X = np.c_[np.ones((len(X),1)), X]\n",
    "Y = np.array(df_inliers['diag_3'].iloc[0:1000])\n",
    "Y_chunk = chunkIt(Y, len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data site A and B (target class Y is at B site ###\n",
    "X_a = b0_X[:,0:3]\n",
    "X_b = b0_X[:,3:6]\n",
    "\n",
    "### At site A ###\n",
    "XaTXa = np.matrix(X_a).T * X_a\n",
    "len_A = len(X_a[0])\n",
    "### At site B\n",
    "XbTXb = np.matrix(X_b).T * X_b\n",
    "len_B = len(X_b[0])\n",
    "\n",
    "### Computation ###\n",
    "# Computation = 2 * (len(X_a[0]) * len(X_b[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### At site A ###\n",
    "A_randoms = []\n",
    "for i in range(0, len_A):\n",
    "    np.random.seed(1)\n",
    "    A_randoms.append(np.random.randint(0,5, len(X_a[:,i])))\n",
    "    \n",
    "C_noises = []    \n",
    "for i in range(0, len_A):\n",
    "    np.random.seed(2)\n",
    "    C_noises.append(np.random.randint(0,5, (len(X_a[:,i]), len(X_a[:,i]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### At site A ###\n",
    "SA_noises = []\n",
    "for i in range(0, len_A):\n",
    "    SA_noises.append(np.add(X_a[:,i], np.dot(C_noises[i], A_randoms[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### At site B ###\n",
    "S_noises = []\n",
    "for i in range(0, len_B):\n",
    "    S_noises_site = []\n",
    "    for j in range(0, len_A):\n",
    "        S_noises_site.append(np.dot(SA_noises[j], X_b[:,i])) # X_b[:,i]\n",
    "    S_noises.append(S_noises_site)\n",
    "    \n",
    "SB_noises = []\n",
    "for i in range(0, len_B): \n",
    "    SB_noise_site = []\n",
    "    for j in range(0, len_A):\n",
    "        SB_noise_site.append(np.dot(C_noises[j].transpose(), X_b[:,i])) # X_b[:,i]\n",
    "    SB_noises.append(SB_noise_site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### At site B ###\n",
    "B_random_singles = []\n",
    "for i in range(0, len_A):\n",
    "    np.random.seed(3)\n",
    "    B_random_singles.append(np.random.randint(0,5, int(len(X_b[:,0])/2))) #in this case r =5 # X_b[:,0]\n",
    "# B_random_singles = np.array(B_random_singles)\n",
    "\n",
    "B_noises_add = []\n",
    "for n in range(0, len_B):\n",
    "    B_noise_2 = []\n",
    "    for i in range(0, len_A):\n",
    "        B_random = []\n",
    "        for j in range(0, len(B_random_singles[i])): \n",
    "            B_random.append(B_random_singles[i][j])\n",
    "            B_random.append(B_random_singles[i][j])\n",
    "        B_noise_2.append(SB_noises[n][i] + B_random)\n",
    "    B_noises_add.append(B_noise_2)\n",
    "#B sends A: Snoise_1 and ynoise_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_sums = []\n",
    "for i in range(0, len_A):\n",
    "    r_sum = 0\n",
    "    for j in range(0, len(B_random_singles[0])):\n",
    "        r_sum = r_sum + (A_randoms[i][2*j] + A_randoms[i][2*j+1]) * B_random_singles[i][j]\n",
    "    rand_sums.append(r_sum)\n",
    "\n",
    "\n",
    "outcomes = []\n",
    "for n in range(0, len_B):\n",
    "    out = []\n",
    "    for i in range(0, len_A):\n",
    "        out.append(S_noises[n][i] - np.dot(A_randoms[i],B_noises_add[n][i]) + rand_sums[i]) \n",
    "    outcomes.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XaTXb = [[4616.0, 26824.0, 6676.0],\n",
    " [319209.0, 1823609.0, 457053.0],\n",
    " [258630.0, 1450363.0, 367140.0]]\n",
    "\n",
    "\n",
    "XbTXa = [[4616.0, 319209.0, 258630.0],\n",
    " [26824.0, 1823609.0, 1450363.0],\n",
    " [6676.0, 457053.0, 367140.0]]\n",
    "\n",
    "XaTY = np.matrix(chunkIt(([285387.0, 1559025.0, 411779.0]),3))\n",
    "XbTY = np.matrix(X_b).T * Y_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_XTX = np.concatenate((np.concatenate((XaTXa, XbTXa), axis=1), np.concatenate((XaTXb, XbTXb), axis=1)),axis=0) \n",
    "pp_XTY = np.concatenate((XaTY, XbTY),axis=0)\n",
    "np.linalg.inv(pp_XTX) * pp_XTY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### correct X.T * X, X.T * Y ###\n",
    "# XTX = np.dot(b0_X.T, b0_X)\n",
    "# XTY = (np.matrix(b0_X).T * Y_chunk) \n",
    "# corr_Out = np.linalg.inv(XTX) * (np.matrix(b0_X).T * Y_chunk) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Checking with the centralized method ####\n",
    "m,c = np.linalg.lstsq(b0_X,Y_chunk)[0:2]\n",
    "print(m,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Checking with the scikit learn method ####\n",
    "regr = linear_model.LinearRegression(fit_intercept=True, normalize=True)\n",
    "regr.fit(X, Y)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "print('Intercept: \\n', regr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
