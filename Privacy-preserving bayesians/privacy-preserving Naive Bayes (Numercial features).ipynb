{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook implements Naive Bayes classification in a privacy-preserving manner. ###\n",
    "In machine learning, naive Bayes classifiers are a family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with strong (naive) independence assumptions between the features. More mathematical explaination is provided by https://en.wikipedia.org/wiki/Naive_Bayes_classifier \n",
    "\n",
    "Applying Naive Bayes in a veritcally partitioned data scenario, we need to consider two different situations:\n",
    "1. Both data sites know the target class\n",
    "2. Only one (part) of data sites know the target class and they are not willing to share\n",
    "    * Features are categorical values\n",
    "    * Features are numerical values\n",
    "    \n",
    "We provide solutions for both situations in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from scipy.special import comb, logsumexp\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Situation 1: Naive Bayes - all parties know target class ###\n",
    "We use Gaussian Naive Bayes to solve numerical features problem. Categorical features can be solved in the same way by using different kernels.\n",
    "Please check https://scikit-learn.org/stable/modules/classes.html#module-sklearn.naive_bayes to find more possible kernels.\n",
    "In this notebook, we assume there are two data parties, semi-honest which means they will follow the protocal and still curious about the other parties data. \n",
    "\n",
    "############ Due to the var_smoothing variable in Scikit Learn model, np.(X, axis=0).max() needs set the same smoothing variable. I spent the whole day to find why the combined model does not equal to the centralized model ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_smoothing_ = 1e-20\n",
    "var_smoothing_trans = (np.var(df_ctr, axis=0).max()) * var_smoothing_ / (np.var(df_A, axis=0).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### At Data site A #####\n",
    "df_A_withTar = pd.DataFrame.from_csv('preprocessed_dataFile_A.csv')\n",
    "TargetClass = df_A_withTar['number_outpatient'] # Target class which is known by both data parties\n",
    "\n",
    "# Data will be input to the model #\n",
    "df_A = df_A_withTar.drop(['number_outpatient'], axis=1)\n",
    "\n",
    "# Fit the Naive Bayes model #\n",
    "gnb_A1 = GaussianNB(var_smoothing=var_smoothing_trans) #### OMG,WTF,AAAAAAAA, I become crazy to find this!!!!!\n",
    "gnb_A1.fit(df_A, TargetClass)\n",
    "\n",
    "# Get the probability estimates #\n",
    "lh_A1 = gnb_A1._joint_log_likelihood(df_A)\n",
    "lhl_A1 = logsumexp(lh_A1, axis=1)\n",
    "PE_A1 = np.exp(lh_A1 - np.atleast_2d(lhl_A1).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### At Data site B #####\n",
    "df_B = pd.DataFrame.from_csv('preprocessed_dataFile_B.csv')\n",
    "\n",
    "gnb_B1 = GaussianNB(var_smoothing = var_smoothing_)\n",
    "gnb_B1.fit(df_B, TargetClass)\n",
    "\n",
    "lh_B1 = gnb_B1._joint_log_likelihood(df_B)\n",
    "lhl_B1 = logsumexp(lh_B1, axis=1)\n",
    "PE_B1 = np.exp(lh_B1 - np.atleast_2d(lhl_B1).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### This part calculation can be done at Trusted Third Party ####\n",
    "lh_D = lh_A1 + lh_B1 - np.log(gnb_A1.class_prior_)\n",
    "lhl_D = logsumexp(lh_D, axis=1)\n",
    "\n",
    "# Outcome is probability estimates #\n",
    "# The instances will be classified to the class which has the highest probability #  \n",
    "PE_D1 = np.exp(lh_D - np.atleast_2d(lhl_D).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.91341644e-10,   4.34641283e-08,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   9.35342940e-01,   0.00000000e+00],\n",
       "       [  2.85991005e-17,   1.56440293e-15,   9.99999395e-01, ...,\n",
       "          2.98338253e-37,   4.75642956e-07,   0.00000000e+00],\n",
       "       [  6.65256317e-06,   1.95695013e-04,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   3.65514993e-04,   0.00000000e+00],\n",
       "       ..., \n",
       "       [  9.99999972e-01,   2.76898111e-08,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  3.12750655e-03,   6.50278181e-02,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  8.04122143e-03,   5.63358836e-02,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Checking the results with centralized dataset #####\n",
    "df_ctr = pd.concat([df_A, df_B], axis=1)\n",
    "\n",
    "gnb_C1 = GaussianNB(var_smoothing = var_smoothing_)\n",
    "gnb_C1.fit(df_ctr, TargetClass)\n",
    "\n",
    "lh_C1 = gnb_C1._joint_log_likelihood(df_ctr)\n",
    "lhl_C1 = logsumexp(lh_C1, axis=1)\n",
    "PE_C1 = np.exp(lh_C1 - np.atleast_2d(lhl_C1).T)\n",
    "PE_C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1990408665951691e-14"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(PE_D1-PE_C1).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Situation 2: Naive Bayes (partially have target class) ###\n",
    "The challenging part here is to calculate mean and variance values for the party who does not have the target class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### At Data Site A which has the target class #####\n",
    "Model can be built locally as target class is available at this data site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "TargetClass = df_A_withTar['number_outpatient']\n",
    "df_A = df_A_withTar.drop(['number_outpatient'], axis=1)\n",
    "\n",
    "gnb_A2 = GaussianNB(var_smoothing = var_smoothing_trans)\n",
    "gnb_A2.fit(df_A, TargetClass)\n",
    "\n",
    "lh_A2 = gnb_A2._joint_log_likelihood(df_A)\n",
    "lhl_A2 = logsumexp(lh_A2, axis=1)\n",
    "PE_A2 = np.exp(lh_A2 - np.atleast_2d(lhl_A2).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_y = np.unique(TargetClass)\n",
    "\n",
    "class_prior = []\n",
    "if len(class_prior) == 0:\n",
    "    # Empirical prior, with sample_weight taken into account\n",
    "    cnt = Counter(TargetClass)\n",
    "    for y_i in unique_y:\n",
    "        class_prior.append(cnt[y_i]/len(TargetClass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "### convert target class to multiple binary classes ###\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(unique_y)\n",
    "conv_TargetClass = lb.transform(TargetClass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### At Data Site B (no TargetClass) ####\n",
    "Firstly, we need to calculate mean value of each features at data site B based on different classes of Target feature.\n",
    "Here we need to use secure scalar product to get the mean(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Starts at data site B #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_a = df_B # .transpose()\n",
    "len_A = len(X_a.columns)\n",
    "\n",
    "# Generate random numbers and add to data at Data Site A\n",
    "A_randoms = []\n",
    "for i in range(0, len_A):\n",
    "    A_randoms.append(np.random.randint(0,5, len(X_a.iloc[:,i])))\n",
    "    \n",
    "C_matrix = [] # C_noises is shared between A and B \n",
    "for i in range(0, len_A):\n",
    "    C_matrix.append(np.random.randint(0,5, (len(X_a.iloc[:,i]), len(X_a.iloc[:,i]))))\n",
    "\n",
    "Sum_noises_A = [] # which will be sent to B\n",
    "for i in range(0, len_A):\n",
    "    Sum_noises_A.append(np.add(X_a.iloc[:,i], np.dot(C_matrix[i], A_randoms[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Site A: send __C_matrix__ and __Sum_noises_A__, __A_randoms_Sumset__ to Data Site B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Site A receive noises from Data site B then do next calculation #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_b = pd.DataFrame.from_records(conv_TargetClass)\n",
    "len_B = len(X_b.columns)\n",
    "B_divide_set = 10\n",
    "\n",
    "Sum_coef_B = []\n",
    "for i in range(0, len_B):\n",
    "    Sum_noises_temp = []\n",
    "    for j in range(0, len_A):\n",
    "        Sum_noises_temp.append(np.dot(C_matrix[j].transpose(), X_b.iloc[:,i])) \n",
    "    Sum_coef_B.append(Sum_noises_temp)\n",
    "\n",
    "B_random_set = []\n",
    "for i in range(0, len_A):\n",
    "#     np.random.seed(3)\n",
    "    B_random_set.append(np.random.randint(0,5, int(len(X_b.iloc[:,0])/B_divide_set))) \n",
    "\n",
    "Sum_noises_B = [] # which will be send to A\n",
    "for n in range(0, len_B):\n",
    "    B_noise = []\n",
    "    for i in range(0, len_A):\n",
    "        B_random_inter = []\n",
    "        for j in range(0, len(B_random_set[i])): \n",
    "            for k in range(0, B_divide_set):\n",
    "                B_random_inter.append(B_random_set[i][j])\n",
    "        B_noise.append(Sum_coef_B[n][i] + B_random_inter)\n",
    "    Sum_noises_B.append(B_noise)\n",
    "\n",
    "# Add noises dataset A to the dataset B\n",
    "Sum_noises_AB = []\n",
    "for i in range(0, len_B):\n",
    "    Sum_noises_temp = []\n",
    "    for j in range(0, len_A):\n",
    "        Sum_noises_temp.append(np.dot(Sum_noises_A[j], X_b.iloc[:,i])) # X_b[:,i]\n",
    "    Sum_noises_AB.append(Sum_noises_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B sends __Sum_noises_B__ to A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Back to Data Site B #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_randoms_Sumset = []\n",
    "for i in range(0, len_A):\n",
    "    sum_temp = []\n",
    "    for j in range(0, int(len(X_a)/B_divide_set)):\n",
    "        temp = 0\n",
    "        for k in range(0, B_divide_set):\n",
    "            temp = temp + A_randoms[i][B_divide_set*j + k]\n",
    "        sum_temp.append(temp)\n",
    "        \n",
    "    A_randoms_Sumset.append(sum_temp)\n",
    "\n",
    "    \n",
    "Sum_noises_B_Arand = []\n",
    "for n in range(0, len_B):\n",
    "    temp = []\n",
    "    for i in range(0, len_A):\n",
    "        temp.append(np.dot(A_randoms[i],Sum_noises_B[n][i]))\n",
    "    Sum_noises_B_Arand.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Move to Data Site A again --> As A has the target feature, A calculate the final results #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_sums = []\n",
    "for i in range(0, len_A):\n",
    "    r_sum = 0\n",
    "    for j in range(0, len(B_random_set[0])):\n",
    "        r_sum = r_sum + A_randoms_Sumset[i][j] * B_random_set[i][j]\n",
    "    rand_sums.append(r_sum)\n",
    "\n",
    "outcomes = []\n",
    "for n in range(0, len_B):\n",
    "    out = []\n",
    "    for i in range(0, len_A):\n",
    "        out.append(Sum_noises_AB[n][i] - Sum_noises_B_Arand[n][i] + rand_sums[i]) \n",
    "    outcomes.append(out)\n",
    "    \n",
    "sum_out = np.array(outcomes)\n",
    "mean = []\n",
    "for s in range(0, len(sum_out)):\n",
    "    mean.append(list(sum_out[s, :]/Counter(TargetClass)[unique_y[s]]))\n",
    "#     print(list(sum_out[s, :]/Counter(TargetClass)[unique_y[s]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### check with normal model### mean = theta_\n",
    "gnb_B2 = GaussianNB(var_smoothing=var_smoothing_)\n",
    "gnb_B2.fit(df_B, TargetClass)\n",
    "(gnb_B2.theta_ - mean).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Securely compute variance ####\n",
    "* Now, we have mean(s). Data Site A needs to send mean(s) to Data Site B to calculate variance.\n",
    "* Let's calculate variance.\n",
    "* Formula: variance = mean(abs(x-x(mean))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### At Data Site B: we calculate abs(x-x(mean)) ** 2 #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_meanSub2 = []\n",
    "for i in range(0, len(mean)):\n",
    "    list_meanSub2.append(np.power(df_B.sub(mean[i], axis='columns'), 2))\n",
    "B_divide_set = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_1(X_a, B_divide_set):\n",
    "    len_A = len(X_a.columns)\n",
    "\n",
    "    # Generate random numbers and add to data at Data Site A\n",
    "    A_randoms = []\n",
    "    for i in range(0, len_A):\n",
    "        A_randoms.append(np.random.randint(0,5, len(X_a.iloc[:,i])))\n",
    "\n",
    "    C_matrix = [] # C_noises is shared between A and B \n",
    "    for i in range(0, len_A):\n",
    "        C_matrix.append(np.random.randint(0,5, (len(X_a.iloc[:,i]), len(X_a.iloc[:,i]))))\n",
    "\n",
    "    Sum_noises_A = [] # which will be sent to B\n",
    "    for i in range(0, len_A):\n",
    "        Sum_noises_A.append(np.add(X_a.iloc[:,i], np.dot(C_matrix[i], A_randoms[i])))\n",
    "        \n",
    "    A_randoms_Sumset = []\n",
    "    for i in range(0, len_A):\n",
    "        sum_temp = []\n",
    "        for j in range(0, int(len(X_a)/B_divide_set)):\n",
    "            temp = 0\n",
    "            for k in range(0, B_divide_set):\n",
    "                temp = temp + A_randoms[i][B_divide_set*j + k]\n",
    "            sum_temp.append(temp)\n",
    "\n",
    "        A_randoms_Sumset.append(sum_temp)\n",
    "    \n",
    "    return C_matrix, Sum_noises_A, A_randoms_Sumset, A_randoms, len_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_matrix_list = []\n",
    "Sum_noises_A_list = []\n",
    "A_randoms_Sumset_list = []\n",
    "A_randoms_list = []\n",
    "\n",
    "# Each class has one matrix of abs(x-x(mean))** 2 \n",
    "# It means Data Site B needs to send n matrix to Data Site A(n = classes)\n",
    "for i in range(0, len(list_meanSub2)):\n",
    "    C_matrix, Sum_noises_A, A_randoms_Sumset, A_randoms, len_A = step_1(list_meanSub2[i], B_divide_set)\n",
    "    C_matrix_list.append(C_matrix)\n",
    "    Sum_noises_A_list.append(Sum_noises_A)\n",
    "    A_randoms_Sumset_list.append(A_randoms_Sumset)\n",
    "    A_randoms_list.append(A_randoms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### B Sends N Noised Matrix to Data Site A #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we assume only one feature is regarded as the target class #\n",
    "def B_Cal (X_b, len_A, C_matrix, Sum_noises_A):\n",
    "    Sum_coef_B = []\n",
    "    for j in range(0, len_A): \n",
    "        Sum_coef_B.append(np.dot(C_matrix[j].transpose(), X_b)) # X_b[:,i]\n",
    "\n",
    "    B_random_set = []\n",
    "    for i in range(0, len_A):\n",
    "    #     np.random.seed(3)\n",
    "        B_random_set.append(np.random.randint(0,5, int(len(X_b)/B_divide_set))) # X_b[:,i]\n",
    "\n",
    "    Sum_noises_B = [] # which will be send to A\n",
    "    for i in range(0, len_A):\n",
    "        B_random_inter = []\n",
    "        for j in range(0, len(B_random_set[i])): \n",
    "            for k in range(0, B_divide_set):\n",
    "                B_random_inter.append(B_random_set[i][j])\n",
    "        Sum_noises_B.append(Sum_coef_B[i] + B_random_inter) # B_noise Sum_coef_B[n][i]\n",
    "\n",
    "    # Add noises dataset A to the dataset B\n",
    "    Sum_noises_AB = []\n",
    "    for j in range(0, len_A):\n",
    "        Sum_noises_AB.append(np.dot(Sum_noises_A[j], X_b)) # X_b[:,i] Sum_noises_temp\n",
    "    \n",
    "    return Sum_noises_B, Sum_noises_AB, B_random_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sum_noises_B_list = []\n",
    "Sum_noises_AB_list = []\n",
    "B_random_set_list = []\n",
    "for i in range(0, len(conv_TargetClass[0])):\n",
    "    Sum_noises_B, Sum_noises_AB, B_random_set = B_Cal(conv_TargetClass[:,i], len_A, \\\n",
    "                                                      C_matrix_list[i], Sum_noises_A_list[i])\n",
    "    Sum_noises_B_list.append(Sum_noises_B)\n",
    "    Sum_noises_AB_list.append(Sum_noises_AB)\n",
    "    B_random_set_list.append(B_random_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Back to Data Site B #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sum_noises_B_Arand_list = []\n",
    "for k in range(0, len(Sum_noises_B_list)):\n",
    "\n",
    "    Sum_noises_B_Arand = []\n",
    "    for i in range(0, len_A):\n",
    "        Sum_noises_B_Arand.append(np.dot(A_randoms_list[k][i],Sum_noises_B_list[k][i])) #temp\n",
    "        \n",
    "    Sum_noises_B_Arand_list.append(Sum_noises_B_Arand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Last stage: Go to Data Site A again to calculate the final results (variance) #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_sums_list = []\n",
    "for n in range(0, len(B_random_set_list)):\n",
    "    rand_sums = []\n",
    "    for i in range(0, len_A):\n",
    "        r_sum = 0\n",
    "        for j in range(0, len(B_random_set_list[n][0])):\n",
    "            r_sum = r_sum + A_randoms_Sumset_list[n][i][j] * B_random_set_list[n][i][j]\n",
    "        rand_sums.append(r_sum)\n",
    "    rand_sums_list.append(rand_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes_var = []\n",
    "for n in range(0, len(Sum_noises_AB_list)):\n",
    "    outcomes = []\n",
    "    for i in range(0, len_A):\n",
    "        outcomes.append(Sum_noises_AB_list[n][i] - Sum_noises_B_Arand_list[n][i] + rand_sums_list[n][i]) # out\n",
    "    outcomes_var.append(outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_out = np.array(outcomes_var)\n",
    "variance_init = []\n",
    "for s in range(0, len(sum_out)):\n",
    "    variance_init.append(list(sum_out[s, :]/Counter(TargetClass)[unique_y[s]]))\n",
    "    \n",
    "epsilon = var_smoothing_ * np.var(df_B, axis=0).max() # needs to provide the biggest variance of the whole dataset\n",
    "variance = np.array(variance_init) + epsilon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.4587448984384537e-11"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### check with centralized model### variance = sigma_\n",
    "# gnb_B2.sigma_\n",
    "# ((variance - gnb_B2.sigma_)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Calculation of Probability Estimate #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(df_B, unique_y, class_prior, mean, variance):\n",
    "    joint_log_likelihood = []\n",
    "    for i in range(np.size(unique_y)):###\n",
    "        jointi = np.log(class_prior[i])\n",
    "        n_ij = - 0.5 * np.sum(np.log(2. * np.pi * variance[i, :]))\n",
    "        n_ij -= 0.5 * np.sum(((df_B - mean[i, :])**2)/(variance[i, :]), 1)\n",
    "        joint_log_likelihood.append(jointi + n_ij)\n",
    "\n",
    "    joint_log_likelihood = np.array(joint_log_likelihood).T\n",
    "    return joint_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "lh_B2 = likelihood(df_B, unique_y, class_prior, np.array(mean), np.array(variance))\n",
    "lhl_B2 = logsumexp(lh_B2, axis=1)\n",
    "PE_B2 = np.exp(lh_B2 - np.atleast_2d(lhl_B2).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.1436760113184903e-10"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### . Check the outcome\n",
    "# (gnb_B2.predict_proba(df_B)-PE_B2).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.34600287e-48,   1.47187714e-35,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   1.00000000e+00,   0.00000000e+00],\n",
       "       [  2.86870118e-61,   1.56588251e-48,   1.00000000e+00, ...,\n",
       "          9.42137512e-43,   1.50357780e-12,   0.00000000e+00],\n",
       "       [  5.77977279e-41,   1.69659483e-28,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   9.99999973e-01,   0.00000000e+00],\n",
       "       ..., \n",
       "       [  3.66234175e-04,   9.99633766e-01,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  3.36642869e-25,   6.98510839e-13,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  8.62149936e-25,   6.02717224e-13,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00]])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### The final step: calculation of probability estimate is better to be done at Trusted Tirth Party ####\n",
    "lh_D2 = lh_A2 + lh_B2 - np.log(gnb_B2.class_prior_)\n",
    "lhl_D2 = logsumexp(lh_D2, axis=1)\n",
    "\n",
    "# Outcome is probability estimates #\n",
    "# The instances will be classified to the class which has the highest probability #  \n",
    "PE_D2 = np.exp(lh_D2 - np.atleast_2d(lhl_D2).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.2377878983757e-11"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Checking the results with centralized dataset #####\n",
    "# df_ctr = pd.concat([df_A, df_B], axis=1)\n",
    "\n",
    "# gnb_C1 = GaussianNB(var_smoothing=var_smoothing_)\n",
    "# gnb_C1.fit(df_ctr, TargetClass)\n",
    "\n",
    "# lh_C1 = gnb_C1._joint_log_likelihood(df_ctr)\n",
    "# lhl_C1 = logsumexp(lh_C1, axis=1)\n",
    "# PE_C1 = np.exp(lh_C1 - np.atleast_2d(lhl_C1).T)\n",
    "# (PE_C1-PE_D2).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Networks ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
    "# data = pd.DataFrame(data={'A': [0, 0, 1], 'B': [0, 1, 0], 'C': [1, 1, 0]})\n",
    "# model = BayesianModel([('A', 'C'), ('B', 'C')])\n",
    "# cpd_A = MaximumLikelihoodEstimator(model, data).estimate_cpd('A')\n",
    "# print(cpd_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.estimators import BdeuScore, K2Score, BicScore\n",
    "from pgmpy.models import BayesianModel\n",
    "from scipy.stats import chisquare\n",
    "bdeu_ESS = 5\n",
    "data = df_A\n",
    "bdeu = BdeuScore(data, equivalent_sample_size=bdeu_ESS)\n",
    "k2 = K2Score(data)\n",
    "bic = BicScore(data)\n",
    " \n",
    "model1 = BayesianModel([('race', 'num_procedures'), ('age', 'num_procedures')])\n",
    "\n",
    "print(bdeu.score(model1))\n",
    "print(k2.score(model1))\n",
    "print(bic.score(model1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Site A with Target Class #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_state_names(df, feature):\n",
    "    state_names = dict()\n",
    "    if isinstance(feature, list): \n",
    "        for c in feature:\n",
    "            values = list(Counter(df[c]).keys())\n",
    "            values.sort()\n",
    "            state_names[c] = values\n",
    "    else:\n",
    "        values = list(Counter(df[feature]).keys())\n",
    "        values.sort()\n",
    "        state_names[feature] = values\n",
    "        \n",
    "    return state_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_names = give_state_names(df_A, B_feature)\n",
    "state_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_feature = 'num_procedures'\n",
    "state_names_A = give_state_names(df_A, A_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_feature = ['race', 'age']\n",
    "state_names_B = give_state_names(df_A, B_feature)\n",
    "state_names_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert target class to binary\n",
    "def toBinary(unique_y, TargetClass):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(unique_y)\n",
    "    conv_TargetClass = lb.transform(TargetClass)\n",
    "    return conv_TargetClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_features_B = []\n",
    "for b in range(0, len(B_feature)):\n",
    "    conv_features_B.append(toBinary(state_names_B[B_feature[b]], df_A[B_feature[b]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume B has two features\n",
    "status_local_matrix = []\n",
    "if len(conv_features_B) > 1:\n",
    "    for row in range(0, len(conv_features_B[0])):\n",
    "        status_local_matrix.append(list(np.concatenate(np.dot((conv_features_B[0][row][np.newaxis]).T, \\\n",
    "                                                  conv_features_B[1][row][np.newaxis]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Secure scalar product ##### status_local_matrix & TargetClass_A\n",
    "### status_local_matrix has to be sent to Data Site A ###\n",
    "X_a = pd.DataFrame.from_records(status_local_matrix) # .transpose()\n",
    "len_A = len(X_a.columns)\n",
    "\n",
    "# Generate random numbers and add to data at Data Site A\n",
    "A_randoms = []\n",
    "for i in range(0, len_A):\n",
    "    A_randoms.append(np.random.randint(0,5, len(X_a.iloc[:,i])))\n",
    "    \n",
    "C_matrix = [] # C_noises is shared between A and B \n",
    "for i in range(0, len_A):\n",
    "    C_matrix.append(np.random.randint(0,5, (len(X_a.iloc[:,i]), len(X_a.iloc[:,i]))))\n",
    "\n",
    "Sum_noises_A = [] # which will be sent to B\n",
    "for i in range(0, len_A):\n",
    "    Sum_noises_A.append(np.add(X_a.iloc[:,i], np.dot(C_matrix[i], A_randoms[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### At Data Site A #####\n",
    "X_b = pd.DataFrame.from_records(toBinary(state_names_A[A_feature], df_A[A_feature]))\n",
    "len_B = len(X_b.columns)\n",
    "B_divide_set = 10\n",
    "\n",
    "Sum_coef_B = []\n",
    "for i in range(0, len_B):\n",
    "    Sum_noises_temp = []\n",
    "    for j in range(0, len_A):\n",
    "        Sum_noises_temp.append(np.dot(C_matrix[j].transpose(), X_b.iloc[:,i])) \n",
    "    Sum_coef_B.append(Sum_noises_temp)\n",
    "\n",
    "B_random_set = []\n",
    "for i in range(0, len_A):\n",
    "#     np.random.seed(3)\n",
    "    B_random_set.append(np.random.randint(0,5, int(len(X_b.iloc[:,0])/B_divide_set))) \n",
    "\n",
    "Sum_noises_B = [] # which will be send to A\n",
    "for n in range(0, len_B):\n",
    "    B_noise = []\n",
    "    for i in range(0, len_A):\n",
    "        B_random_inter = []\n",
    "        for j in range(0, len(B_random_set[i])): \n",
    "            for k in range(0, B_divide_set):\n",
    "                B_random_inter.append(B_random_set[i][j])\n",
    "        B_noise.append(Sum_coef_B[n][i] + B_random_inter)\n",
    "    Sum_noises_B.append(B_noise)\n",
    "\n",
    "# Add noises dataset A to the dataset B\n",
    "Sum_noises_AB = []\n",
    "for i in range(0, len_B):\n",
    "    Sum_noises_temp = []\n",
    "    for j in range(0, len_A):\n",
    "        Sum_noises_temp.append(np.dot(Sum_noises_A[j], X_b.iloc[:,i])) # X_b[:,i]\n",
    "    Sum_noises_AB.append(Sum_noises_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Back to Data Site B #####\n",
    "A_randoms_Sumset = []\n",
    "for i in range(0, len_A):\n",
    "    sum_temp = []\n",
    "    for j in range(0, int(len(X_a)/B_divide_set)):\n",
    "        temp = 0\n",
    "        for k in range(0, B_divide_set):\n",
    "            temp = temp + A_randoms[i][B_divide_set*j + k]\n",
    "        sum_temp.append(temp)\n",
    "        \n",
    "    A_randoms_Sumset.append(sum_temp)\n",
    "\n",
    "    \n",
    "Sum_noises_B_Arand = []\n",
    "for n in range(0, len_B):\n",
    "    temp = []\n",
    "    for i in range(0, len_A):\n",
    "        temp.append(np.dot(A_randoms[i],Sum_noises_B[n][i]))\n",
    "    Sum_noises_B_Arand.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### At Data Site A to calculate final result #####\n",
    "rand_sums = []\n",
    "for i in range(0, len_A):\n",
    "    r_sum = 0\n",
    "    for j in range(0, len(B_random_set[0])):\n",
    "        r_sum = r_sum + A_randoms_Sumset[i][j] * B_random_set[i][j]\n",
    "    rand_sums.append(r_sum)\n",
    "\n",
    "outcomes = []\n",
    "for n in range(0, len_B):\n",
    "    out = []\n",
    "    for i in range(0, len_A):\n",
    "        out.append(Sum_noises_AB[n][i] - Sum_noises_B_Arand[n][i] + rand_sums[i]) \n",
    "    outcomes.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outcomes_df = pd.DataFrame.from_records(outcomes)\n",
    "header_list = []\n",
    "for i in range(0, len(B_feature)):\n",
    "    header_list.append(state_names_B[B_feature[i]])\n",
    "header = pd.MultiIndex.from_product(header_list,names=B_feature)\n",
    "outcomes_state_counts_df = pd.DataFrame.from_records(outcomes, columns=header)\n",
    "outcomes_state_counts_df.index.name = A_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K2: \n",
    "# k2 = K2Score(data)\n",
    "# print(k2.local_score('num_procedures', parents=['race', 'age']))\n",
    "\n",
    "from math import lgamma, log\n",
    "    \n",
    "var_states = state_names_A[A_feature]\n",
    "var_cardinality = len(var_states)\n",
    "state_counts = outcomes_state_counts_df\n",
    "\n",
    "score = 0\n",
    "for parents_state in outcomes_state_counts_df:  # iterate over df columns (only 1 if no parents)\n",
    "    \n",
    "    conditional_sample_size = sum(outcomes_state_counts_df[parents_state])\n",
    "    score += lgamma(var_cardinality_A) - lgamma(conditional_sample_size + var_cardinality_A)\n",
    "\n",
    "    for state in var_states:\n",
    "        if outcomes_state_counts_df[parents_state][state] > 0:\n",
    "            score += lgamma(outcomes_state_counts_df[parents_state][state] + 1)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize(feature, name):\n",
    "    if type(feature) == pd.core.series.Series:\n",
    "        rep_var = []\n",
    "        uni_var = list(Counter(feature).keys())\n",
    "        random_var = np.random.sample(len(uni_var)) #list(Counter(data['WEALTH_INDEX']).keys()) #\n",
    "        print('%s unique values are in the feature %s' %(len(Counter(feature)), name))\n",
    "        for i in feature:\n",
    "            position = uni_var.index(i)\n",
    "            rep_var.append(random_var[position])\n",
    "    else:\n",
    "        rep_var = []\n",
    "        colomn = list(Counter(feature))\n",
    "        for col in colomn:\n",
    "            slg_var = []\n",
    "            uni_var = list(Counter(feature[col]).keys())\n",
    "            random_var = np.random.sample(len(uni_var)) #list(Counter(data['WEALTH_INDEX']).keys()) #\n",
    "            print('%s unique values are in the features -- %s' %(len(uni_var), col))\n",
    "            for i in feature[col]:\n",
    "                position = uni_var.index(i)\n",
    "                slg_var.append(random_var[position])\n",
    "            rep_var.append(slg_var)\n",
    "    return rep_var\n",
    "\n",
    "def transform(rep_var):\n",
    "    var_states = list(Counter(rep_var).keys()) # A site\n",
    "    var_list = []\n",
    "    for v in var_states:\n",
    "        var_num = []\n",
    "        for i in rep_var:\n",
    "            if i == v:\n",
    "                var_num.append(1)\n",
    "            else:\n",
    "                var_num.append(0)\n",
    "        var_list.append(var_num) # send to B site\n",
    "    var_cardinality = len(var_list) # send to B site\n",
    "    return var_states, var_list, var_cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_varA = randomize(df_A[A_feature], A_feature)\n",
    "rep_varB = []\n",
    "for b in B_feature:\n",
    "    rep_varB.append(randomize(df_A[b], b))\n",
    "rep_varB = np.array(rep_varB)\n",
    "var_states, var_list_A, var_cardinality_A = transform(rep_varA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_feature = 'num_procedures'\n",
    "B_feature = ['race', 'age']\n",
    "rep_varA = randomize(df_A[A_feature], A_feature)\n",
    "rep_varB = []\n",
    "for b in B_feature:\n",
    "    rep_varB.append(randomize(df_A[b], b))\n",
    "rep_varB = np.array(rep_varB)\n",
    "var_states, var_list_A, var_cardinality_A = transform(rep_varA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_counts(var_list_A, rep_varB, B_feature, b):\n",
    "    state_counts = []\n",
    "    \n",
    "    for var in var_list_A: \n",
    "        cnt_varB = []\n",
    "        list_varB = []\n",
    "        for i in range(0, len(var)):\n",
    "            if var[i] == 1:\n",
    "                cnt_varB.append(list(rep_varB[:, i]))\n",
    "        # Second layer\n",
    "        list_sec = []\n",
    "        key_sec = []\n",
    "        cnt_varB = pd.DataFrame.from_records(cnt_varB)\n",
    "        cnt_varB.columns = B_feature\n",
    "        sec_keys = Counter(cnt_varB[B_feature[b]]).keys() ######\n",
    "        for key in sec_keys:\n",
    "            dic_counter = Counter(cnt_varB.loc[cnt_varB[B_feature[b]] == key][B_feature[1]])\n",
    "            for k in dic_counter.keys():\n",
    "                if k not in key_sec: \n",
    "                    key_sec.append(k)\n",
    "            list_sec.append(dic_counter)\n",
    "        # Counter 的顺序要一样\n",
    "        list_key = list(Counter(cnt_varB[B_feature[b]]).keys())\n",
    "        list_values = list(Counter(cnt_varB[B_feature[b]]).values())\n",
    "        for r in Counter(rep_varB[b]).keys():\n",
    "            if r in list_key: \n",
    "                list_varB.append(list_values[list_key.index(r)])\n",
    "            else:\n",
    "                list_varB.append(0)\n",
    "        state_counts.append(list_varB)\n",
    "        \n",
    "    return state_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_counts_list = []\n",
    "for b in range(0, len(B_feature)):\n",
    "    state_counts = get_state_counts(var_list_A, rep_varB, B_feature, b)\n",
    "    state_counts_list.append(state_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_cardinality_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_counts_df_list = []\n",
    "for s in range(0, len(state_counts_list)):\n",
    "    state_counts_df = pd.DataFrame.from_records(state_counts_list[s])\n",
    "    state_counts_df.columns = list(Counter(rep_varB[s]).keys())\n",
    "    state_counts_df.index = var_states\n",
    "    state_counts_df_list.append(state_counts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K2: \n",
    "k2 = K2Score(data)\n",
    "# model1 = BayesianModel([('num_procedures', 'race')])\n",
    "# print(k2.score(model1))\n",
    "print(k2.local_score('num_procedures', parents=['age', 'race']))\n",
    "\n",
    "from math import lgamma, log\n",
    "score = 0\n",
    "for parents_state in state_counts_df_list[1]:  # iterate over df columns (only 1 if no parents)\n",
    "    conditional_sample_size = sum(state_counts_df_list[1][parents_state])\n",
    "\n",
    "    score += lgamma(var_cardinality_A) - lgamma(conditional_sample_size + var_cardinality_A)\n",
    "\n",
    "    for state in var_states:\n",
    "        if state_counts_df_list[1][parents_state][state] > 0:\n",
    "            score += lgamma(state_counts_df_list[1][parents_state][state] + 1)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_names = give_state_names(df_A)\n",
    "variable = 'num_procedures'\n",
    "parents = ['age', 'race']\n",
    "\n",
    "parents_states = [state_names[parent] for parent in parents]\n",
    "\n",
    "# count how often each state of 'variable' occured, conditional on parents' states\n",
    "state_count_data = df_A.groupby([variable] + parents).size().unstack(parents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isinstance(state_count_data.columns, pd.MultiIndex):\n",
    "    state_count_data.columns = pd.MultiIndex.from_arrays([state_count_data.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_index = state_names[variable]\n",
    "column_index = pd.MultiIndex.from_product(parents_states, names=parents)\n",
    "state_counts = state_count_data.reindex(index=row_index, columns=column_index).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k2.state_counts('num_procedures', parents = ['age', 'race'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Secure Sum ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.array([[0,0,0,0,0,0],[2,2,2,2,2,2],[3,3,3,3,3,3]])\n",
    "\n",
    "k = 2\n",
    "n = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkIt(seq, num):\n",
    "    avg = len(seq) / float(num)\n",
    "    out = []\n",
    "    last = 0.0\n",
    "\n",
    "    while last < len(seq):\n",
    "        out.append(seq[int(last):int(last + avg)])\n",
    "        last += avg\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB = []\n",
    "for i in range(0, len(D)):\n",
    "    DB.append(chunkIt(D[i], k))\n",
    "DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.zeros((3, 2, 3))\n",
    "\n",
    "for rc in range(k, 0, -1):\n",
    "    for j in range(0, k):\n",
    "        for i in range(0, n):\n",
    "            s[i][j] = DB[i][j] + s[i][j]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
