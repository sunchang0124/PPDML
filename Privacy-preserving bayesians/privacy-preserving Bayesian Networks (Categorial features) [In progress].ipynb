{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Networks ###\n",
    "For now, I only implemented K2 algorithm as the scoring function in categorial datasets.\n",
    "Please find more information about pgmpy package: http://pgmpy.org/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import lgamma, log\n",
    "from collections import Counter\n",
    "from pgmpy.models import BayesianModel\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from pgmpy.estimators import MaximumLikelihoodEstimator, K2Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structre Learning ###\n",
    "learn model structure (a DAG) from a data set, there are two broad techniques:\n",
    "\n",
    "* score-based structure learning\n",
    "* constraint-based structure learning\n",
    "The combination of both techniques allows further improvement:\n",
    "\n",
    "* hybrid structure learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score-based Structure Learning ####\n",
    "This approach construes model selection as an optimization task. It has two building blocks:\n",
    "\n",
    "* A scoring function $s_D\\colon M \\to \\mathbb R$ that maps models to a numerical score, based on how well they fit to a given data set $D$.\n",
    "* A search strategy to traverse the search space of possible models $M$ and select a model with optimal score.\n",
    "\n",
    "##### Scoring functions #####\n",
    "Commonly used scores to measure the fit between model and data are Bayesian Dirichlet scores such as BDeu or K2 and the Bayesian Information Criterion (BIC, also called MDL). See [1], Section 18.3 for a detailed introduction on scores. As before, BDeu is dependent on an equivalent sample size.\n",
    "\n",
    "In this code for now, I only implemented K2 algorithm as the scoring function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Only data site A has the target class #####\n",
    "In this case, I would like to use a Trusted Third Party to do the final calculation. It is fine to do it at one of data parties, but the states table leaks some information about data (statistical summary level). If we do that in the TTP, then TTP does not know anything about data itself even not the column name. TTP only needs to do one calculation without knowing anything. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k2 = K2Score(data)\n",
    "# model1 = BayesianModel([('race', 'num_procedures'), ('age', 'num_procedures')]) # race -> num_procedures <- age \n",
    "# print(k2.score(model1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_state_names(df, feature):\n",
    "    state_names = dict()\n",
    "    if isinstance(feature, list): \n",
    "        for c in feature:\n",
    "            values = list(Counter(df[c]).keys())\n",
    "            values.sort()\n",
    "            state_names[c] = values\n",
    "    else:\n",
    "        values = list(Counter(df[feature]).keys())\n",
    "        values.sort()\n",
    "        state_names[feature] = values\n",
    "        \n",
    "    return state_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert target class to binary\n",
    "def toBinary(unique_y, TargetClass):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(unique_y)\n",
    "    conv_TargetClass = lb.transform(TargetClass)\n",
    "    return conv_TargetClass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### At Data Site A with Target Class #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_procedures': [0, 1, 2, 3, 4, 5, 6]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_A = pd.DataFrame.from_csv('preprocessed_dataFile_A.csv').drop('num_lab_procedures', axis=1)\n",
    "A_feature = 'num_procedures'\n",
    "state_names_A = give_state_names(df_A, A_feature)\n",
    "state_names_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### At Data Site B without Target Class #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A1Cresult': [0, 1, 2, 3], 'max_glu_serum': [0, 1, 2, 3]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_B = pd.DataFrame.from_csv('preprocessed_dataFile_B.csv').drop(['diag_1','diag_2','diag_3'], axis=1)\n",
    "B_feature = ['max_glu_serum', 'A1Cresult']\n",
    "state_names_B = give_state_names(df_B, B_feature)\n",
    "state_names_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_features_B = []\n",
    "for b in range(0, len(B_feature)):\n",
    "    conv_features_B.append(toBinary(state_names_B[B_feature[b]], df_B[B_feature[b]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume B has two features\n",
    "status_local_matrix = []\n",
    "if len(conv_features_B) > 1:\n",
    "    for row in range(0, len(conv_features_B[0])):\n",
    "        status_local_matrix.append(list(np.concatenate(np.dot((conv_features_B[0][row][np.newaxis]).T, \\\n",
    "                                                  conv_features_B[1][row][np.newaxis]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secure scalar product ### \n",
    "\n",
    "status_local_matrix with noises has to be sent to Data Site A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### At Data Site B (add noises to the matrix) #####\n",
    "X_a = pd.DataFrame.from_records(status_local_matrix) # .transpose()\n",
    "len_A = len(X_a.columns)\n",
    "\n",
    "# Generate random numbers and add to data at Data Site A\n",
    "A_randoms = []\n",
    "for i in range(0, len_A):\n",
    "    A_randoms.append(np.random.randint(0,5, len(X_a.iloc[:,i])))\n",
    "    \n",
    "C_matrix = [] # C_noises is shared between A and B \n",
    "for i in range(0, len_A):\n",
    "    C_matrix.append(np.random.randint(0,5, (len(X_a.iloc[:,i]), len(X_a.iloc[:,i]))))\n",
    "\n",
    "Sum_noises_A = [] # which will be sent to B\n",
    "for i in range(0, len_A):\n",
    "    Sum_noises_A.append(np.add(X_a.iloc[:,i], np.dot(C_matrix[i], A_randoms[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Site A receives noised B_Matrix #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_b = pd.DataFrame.from_records(toBinary(state_names_A[A_feature], df_A[A_feature]))\n",
    "len_B = len(X_b.columns)\n",
    "B_divide_set = 10\n",
    "\n",
    "Sum_coef_B = []\n",
    "for i in range(0, len_B):\n",
    "    Sum_noises_temp = []\n",
    "    for j in range(0, len_A):\n",
    "        Sum_noises_temp.append(np.dot(C_matrix[j].transpose(), X_b.iloc[:,i])) \n",
    "    Sum_coef_B.append(Sum_noises_temp)\n",
    "\n",
    "B_random_set = []\n",
    "for i in range(0, len_A):\n",
    "#     np.random.seed(3)\n",
    "    B_random_set.append(np.random.randint(0,5, int(len(X_b.iloc[:,0])/B_divide_set))) \n",
    "\n",
    "Sum_noises_B = [] # which will be send to A\n",
    "for n in range(0, len_B):\n",
    "    B_noise = []\n",
    "    for i in range(0, len_A):\n",
    "        B_random_inter = []\n",
    "        for j in range(0, len(B_random_set[i])): \n",
    "            for k in range(0, B_divide_set):\n",
    "                B_random_inter.append(B_random_set[i][j])\n",
    "        B_noise.append(Sum_coef_B[n][i] + B_random_inter)\n",
    "    Sum_noises_B.append(B_noise)\n",
    "\n",
    "# Add noises dataset A to the dataset B\n",
    "Sum_noises_AB = []\n",
    "for i in range(0, len_B):\n",
    "    Sum_noises_temp = []\n",
    "    for j in range(0, len_A):\n",
    "        Sum_noises_temp.append(np.dot(Sum_noises_A[j], X_b.iloc[:,i])) # X_b[:,i]\n",
    "    Sum_noises_AB.append(Sum_noises_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Back to Data Site B #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_randoms_Sumset = []\n",
    "for i in range(0, len_A):\n",
    "    sum_temp = []\n",
    "    for j in range(0, int(len(X_a)/B_divide_set)):\n",
    "        temp = 0\n",
    "        for k in range(0, B_divide_set):\n",
    "            temp = temp + A_randoms[i][B_divide_set*j + k]\n",
    "        sum_temp.append(temp)\n",
    "        \n",
    "    A_randoms_Sumset.append(sum_temp)\n",
    "\n",
    "    \n",
    "Sum_noises_B_Arand = []\n",
    "for n in range(0, len_B):\n",
    "    temp = []\n",
    "    for i in range(0, len_A):\n",
    "        temp.append(np.dot(A_randoms[i],Sum_noises_B[n][i]))\n",
    "    Sum_noises_B_Arand.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### At Data Site A to calculate final result #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_sums = []\n",
    "for i in range(0, len_A):\n",
    "    r_sum = 0\n",
    "    for j in range(0, len(B_random_set[0])):\n",
    "        r_sum = r_sum + A_randoms_Sumset[i][j] * B_random_set[i][j]\n",
    "    rand_sums.append(r_sum)\n",
    "\n",
    "outcomes = []\n",
    "for n in range(0, len_B):\n",
    "    out = []\n",
    "    for i in range(0, len_A):\n",
    "        out.append(Sum_noises_AB[n][i] - Sum_noises_B_Arand[n][i] + rand_sums[i]) \n",
    "    outcomes.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_list = []\n",
    "for i in range(0, len(B_feature)):\n",
    "    header_list.append(state_names_B[B_feature[i]])\n",
    "header = pd.MultiIndex.from_product(header_list,names=B_feature)\n",
    "outcomes_state_counts_df = pd.DataFrame.from_records(outcomes, columns=header)\n",
    "outcomes_state_counts_df.index.name = A_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final scoring #####\n",
    "* The following part is suggested to be done at Trusted Third Party because the states table is released.\n",
    "* If TTP does not exist, this can be done at Data Site A who has the target class. (comparing to Data Site B does the final calculation, A is safer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5041.432981647914\n"
     ]
    }
   ],
   "source": [
    "var_states = state_names_A[A_feature]\n",
    "var_cardinality = len(var_states)\n",
    "state_counts = outcomes_state_counts_df\n",
    "\n",
    "score = 0\n",
    "for parents_state in outcomes_state_counts_df:  # iterate over df columns (only 1 if no parents)\n",
    "    \n",
    "    conditional_sample_size = sum(outcomes_state_counts_df[parents_state])\n",
    "    score += lgamma(var_cardinality) - lgamma(conditional_sample_size + var_cardinality)\n",
    "\n",
    "    for state in var_states:\n",
    "        if outcomes_state_counts_df[parents_state][state] > 0:\n",
    "            score += lgamma(outcomes_state_counts_df[parents_state][state] + 1)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5041.432981647914\n"
     ]
    }
   ],
   "source": [
    "# # Check with centralized K2 algorithm: \n",
    "# df_ctr = pd.concat([df_A, df_B], axis=1)\n",
    "# k2 = K2Score(df_ctr)\n",
    "# print(k2.local_score(A_feature, parents=B_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
